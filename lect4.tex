%
% This is the LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing.  When preparing
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,graphicx}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS231n: CNNs for Visual Recognition
        \hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture 4  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Course Coordinator: Prof. Fei-Fei Li \hfill Scribes: Akash Gupta} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1}{Lecture #1}
%
%   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}
%
%   {\bf Disclaimer}: {\it These notes have not been subjected to the
%   usual scrutiny reserved for formal publications.  They may be distributed
%   outside this class only with the permission of the Course Coordinator.}
%   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
            \vspace{#2}
            \begin{center}
            Figure \thelecnum.#1:~#3
            \end{center}
    }
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{4}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.
- \textbf{Computational graphs}

\begin{itemize}
\item Graphical representation for a function where nodes of the graph are steps in between for processing.
\item \underline{Advantage}: Allows us to use backpropagation which involves recursively using chain-rule in order to compute the gradients.

\item Each node has local inputs, local gradients, output and Gradients.

\end{itemize}

Q) \textit{Why use a computational graph to calculate a gradient when we can just calculate the derivative?}
\\
A) Calculating the derivative of the expression becomes really difficult for complex expressions. Computational graphs solve this by representing these expressions in simpler form and then just multipying the results using the chain rule:
\[
\dfrac{\partial L}{\partial x} = \dfrac{\partial L}{\partial q}.\dfrac{\partial q}{\partial x}
\]
\begin{itemize}
    \item So, at a node we just want to calculate the local gradients and then during backprop we take numerical value coming from the upstream and multiply with the local gradients and sent these back to the connected nodes without caring about anything else but immediate surroundings.
\end{itemize}

\underline{Patterns in backward flow}\\
- add gate: gradient distributor 
- max gate: gradient router \\
- mul gate: gradient switcher
\\

\underline{Gradients add at branches}:
\[
\displaystyle \dfrac{\partial L}{\partial x} = \sum_i \dfrac{\partial L}{\partial q_i}.\dfrac{\partial q_i}{\partial x}
\]
\underline{Gradients for vectorized code}: The difference is that each variable is now a vector so instead of derivatives there are Jacobian matrics containing derivatives of elements one vector w.r.t. elements of the vector. 

**\underline{NOTE}: The gradient with respect to the variable should have the same shape as the variable.
\\

\underline{API}: 
\begin{itemize}
    \item $forward()$ - compute result of an operation and save any intermediates needed for gradient computation in memory.
    \item $backward()$ - apply chain rule to compute the gradient of the loss function with respect to the inputs.
\end{itemize}


** DL libraries for e.g. Caffe has layers which are basically computational nodes each having a forward pass and backward pass.
\\

- \textbf{Neural Networks}

\begin{itemize}
    \item They are basically classic functions where we have simpler functions that are hierarchically stacked on top of each other to form a more complex non-linear function.
    \item So this is the idea of having multiple stages of having hierarchical computation.
    \item Generally done by stacking linear layers on top each other with non-linear functions in between.
    \item W1(first layer) - templates, W2(second layer) - weighted sum of templates,  x(inputs) - image, h(hidden units) - score of how much of each template of W1 is present in x.
    \item Remember car color example for intuition.
    \item Neural networks are not really neural.
\end{itemize}
\\
\underline{Activation functions}: Sigmoid, leaky RELU, tanh, Maxout, RELU, ELU,...

% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}