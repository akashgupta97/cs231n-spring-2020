%
% This is the LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing.  When preparing
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%

\documentclass[twoside]{article}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,graphicx}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS231n: CNNs for Visual Recognition
        \hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture 3  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Course Coordinator: Prof. Fei-Fei Li \hfill Scribes: Akash Gupta} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
%
%   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}
%
%   {\bf Disclaimer}: {\it These notes have not been subjected to the
%   usual scrutiny reserved for formal publications.  They may be distributed
%   outside this class only with the permission of the Course Coordinator.}
%   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
            \vspace{#2}
            \begin{center}
            Figure \thelecnum.#1:~#3
            \end{center}
    }
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{3}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

- \textbf{Multi - class SVM for image classification} :

\begin{itemize}
    \item Multiclass SVM Loss (Hinge Loss): $L_i = \displaystyle \sum_{j\neq y_i} max(0, s_j - s_{y_i} + 1)$
    \item L = $ \displaystyle \dfrac{1}{N}\sum_{i=1}^{N}L_i$
\end{itemize}
\\
Q) \textit{Why would you ever consider using a squared loss instead of a non-squared loss?}
\\
A) Depends on how much we care about different categories of errors. If an example has been as classified as bad (huge value of loss), using squared loss will square this loss - so really really bad.
Whereas using Hinge loss will not make much difference.

\\
Q) \textit{Are there multiple Ws for which L = 0? If Yes, how to choose W?}
\\
A) Yes, 2W also gives L = 0. We need such W so that the classifier performs well on the test data. Sometimes W fits too well on the training data but not on test data (Case where we need a simpler classifier for test examples). We solve this by adding the regularization term:
\[
\displaystyle L(W) = \dfrac{1}{N} \sum_{i=1}^N L_i(f(x_i, W), y_i) + \lambda R(W)
\]
** Occam's Razor - "\textit{Among competing hypothesis, the simplest is the best}"

- \underline{Types of regularization}: L1, L2, Elastic net (L1 + L2), Max Norm, Dropout, Batch Norm, Stochastic depth

\\
Q) \textit{x = [1,1,1,1], Two Ws given W1: [1,0,0,0], W2: [0.25,0.25,0.25,0.25]. Which one should L2 regression prefer?}
\\
A) It will prefer W2 since the L2 norm is smaller. The idea for L2 regularization is kinda spreading the influence of weights over all the Xs rather than depending on only few Ws. On the contrary, L1 regularization has opposite interpretation where more 0s in W(less parameters) correspond to a simpler model. L1 prefers sparse solution whereas L2 prefers spreading over all the Xs to get a less complex model.
\\

- \textbf {(Softmax Classifier) Multinomial Logistic Regression}:
\[
\displaystyle L_i = -log(\dfrac{e^{s_{y_i}}}{\sum_j e^{s_j}})
\]
\begin{itemize}
    \item min value = 0 (when normalized prob = 1) - practically not possible
    \item max value = 0 (when normalized prob = 0) - practically not possible
\end{itemize}

- \textbf{Optimization}:

Various methods - 

\begin{itemize}
    \item Random search - really bad, don't use
    \item Follow the slope $\rightarrow$ calculate the derivative $\rightarrow$ calculate gradient (vector of partial derivatives) for multiple dimensions $\rightarrow$ Numerical Gradient calculation using finite differences method $\rightarrow$ really slow, but a really good debugging tool (Gradient checking)
    \item Analytic gradient - exact, fast, error - prone
    \begin{itemize}
        \item Vanilla Gradient Descent.
        \item Gradient Descent with Momentum.
        \item Adam Optimizer
        \item and many more....
    \end{itemize}
    \item Use Mini-batch GD as it is much faster than plain GD. In plain GD, one update to weights has to take a whole pass through the dataset which is time-consuming. Favourable values of batches - in powers of 2 (64, 128, 256,....)
\end{itemize}

\underline{Note}: Checkout the interactive Web Demo for better intuition on above concepts.

- \textbf{Image features}:
\begin{itemize}
    \item Feeding raw pixels to linear classifiers is not a good idea instead do some feature representation tasks and then feed it there.
    \item E.g. - Histogram of Oriented Gradients(HOGs) (histogram of edges in the image) - common for object recognition
    \item E.g. - Bag of Words(Extract random patches $\rightarrow$ cluster them $\rightarrow$ create histogram of visual words (codebook) $\rightarrow$ BoWs) - common in NLP.
\end{itemize}

- \underline{Pipeline}: Image $\rightarrow$ Image Feature Extraction $\rightarrow$ Linear Classifier training $\rightarrow$ Prediction. 
\\

- \underline{Teaser}: CNNs are not much different than this as everything is combined into CNN training.

% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}