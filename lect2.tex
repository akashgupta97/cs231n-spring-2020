%
% This is the LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing.  When preparing
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,graphicx}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS231n: CNNs for Visual Recognition
        \hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture 2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Course Coordinator: Prof. Fei-Fei Li \hfill Scribes: Akash Gupta} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1}{Lecture #1}
%
%   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}
%
%   {\bf Disclaimer}: {\it These notes have not been subjected to the
%   usual scrutiny reserved for formal publications.  They may be distributed
%   outside this class only with the permission of the Course Coordinator.}
%   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
            \vspace{#2}
            \begin{center}
            Figure \thelecnum.#1:~#3
            \end{center}
    }
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{2}{}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\begin{itemize}
\item \textbf{Image Classification:} A core problem in Computer Vision.
\item \textbf{The Problem:} Semantic Gap
\item \textbf{Challenges involved:} 
\begin{itemize}
    \item Viewpoint variation
    \item Illumination
    \item Deformation
    \item Occlusion
    \item Background clutter
    \item Intraclass variation
\end{itemize}
\end{itemize}

\underline{Data - Driven approach}:

\begin{enumerate}
    \item Collect a dataset of images and labels.
    \item Use ML to train a classifier.
    \item Evaluate the classifier on new/test images.
\end{enumerate}

API has two components: train() and predict()

\begin{itemize}
\item \underline{train()}: Memorize all data and labels.
\item \underline{predict()}: Predict the label of the most similar training image.
\end{itemize}

Image Classification example - 
\begin{itemize}
\item \underline{Dataset}: CIFAR10 (10 classes, 50k training images, 10k testing images)
\item \underline{K- Nearest Neighbour Classifier}: Fast at training time (O(1)) and Slow at testing time (O(N))
\item \underline{Distance Metric}: L1(Manhattan) distance - $d_1(I_1,I_2) = \displaystyle \sum_p |I^p_1 - I^p_2|$
\item \underline{Distance Metric}: L2(Euclidean) distance - $d_1(I_1,I_2) = \displaystyle \sqrt{\sum_p (I^p_1 - I^p_2)^2}
$

- L2 distance is invariant to rotation of the coordinate axes as contrary to L1.

\item \underline{Hyperparameters}: K, distance metric
- Choices that we set rather than learn.
\end{itemize}

Q) \textit{Where is L1 distance preferred over L2 distance?} \\
A) Best ans is it's mainly problem dependent so try both and see what works! L1 has coordinate dependency. Assuming a case that if individual elements in a vector have some meaning (like certain features) then L1 might be better. 

- Setting hyperparameters - 
\begin{itemize}
    \item Best idea is to split the data in 3 sets: train, val and test. Choose hyperparameters on val and evaluate on test set.
    \item \underline {Cross - Validation:} Split data into K folds and try each fold as validation set and average the results.
\end{itemize}

Q) \textit{What if test data is not representative of the data out in the wild?}\\
A) Generally does not happen since dataset is curated from a same probability distribution. While splitting, always partition randomly to avoid these type of shifts.

- kNN never used for classifying images - very slow at test time, distance metrics on pixels are not informative, curse of dimensionality since kNN can only work if the training set is densely populated. 

- Linear classification:
\begin{itemize}
\item Take an image(X) and set of parameters (W) and input it to a function $f(x, W) = Wx + b$  and output a vector of 10 scores with each score signifying the probability of the image to belong to the respective class.

\item Similar to \textbf{template matching} where each row of the weight matrix is sorta learned template of the images in the training dataset.
\end{itemize}

\underline{Assignment-1}\\
\begin{itemize}
\item to extract the maximum occurring element in an array - 
\begin{itemize}
\item counts =  np.bincount(a) \\
y\_pred[i] = np.argmax(counts)


\end{itemize}
\item Python broadcasting - 
\begin{itemize}
    \item (4,) + (3,4) = (3,4)
    \item (4,) + (4,3) - ERROR: Cannot broadcast together
\end{itemize}

\end{itemize}

\textbf{Inline Questions:}

Q) \textit{Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)}

\begin{itemize}
    \item What in the data is the cause behind the distinctly bright rows?
    \item What causes the columns?
    \item \textcolor{blue}{YourAnswer}:  The high value of the L2 distance calculated. Outliers in the dataset.
\end{itemize}

Q) \textit{Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.}
\begin{itemize}
\item Subtracting the mean  $\mu(\Tilde{p}^{(k)}_{ij}=p^{(k)}_{ij} - \mu)$
\item Subtracting the per pixel mean  $\mu_{ij}(\Tilde{p}^{(k)}_{ij}=p^{(k)}_{ij} - \mu_{ij})$
\item Subtracting the mean  $\mu$  and dividing by the standard deviation  $\sigma$.
\item Subtracting the pixel-wise mean  $\mu_{ij}$  and dividing by the pixel-wise standard deviation  $\sigma_{ij}$.

\item Rotating the coordinate axes of the data.

\item \textcolor{blue}{YourAnswer}:  1,2, 3,4

\item \textcolor{blue}{YourExplanation}:  1)No change on subtracting the mean as all pixels are shifted by the same amount in data space.

2) Same with standard as all shifted points are scaled by same amount so distance also scaled by same amount. So biggest still remains biggest.

3) Rotating does change performance as relative distance does not remains the same.

\end{itemize}

% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}